{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_chal= pd.read_csv('train.csv')\n",
    "test_chal = pd.read_csv('test.csv')\n",
    "chall_details= pd.read_csv('challenge_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train_chal.groupby('user_id').challenge.apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=test_chal.groupby('user_id').challenge.apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.user_id).intersection(test.user_id\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69532, 2), (39732, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_list=pd.concat((train.challenge,test.challenge)).apply(lambda x: x.split())\n",
    "# sentence_list= train.challenge.apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing_15_5_100\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model to learn embeddings\n",
    "neg=15\n",
    "itera=100\n",
    "window=5\n",
    "size=500\n",
    "print 'doing_'+str(neg)+'_'+str(window)+'_'+str(itera)\n",
    "model=gensim.models.Word2Vec(sentence_list,size=size,window=window,workers=8,min_count=0, hs=1,iter=itera,sg=1)\n",
    "model.save('word2vec_solved_iter__noshuf_5window_'+ str(itera) + '_size_' + str(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model= gensim.models.Word2Vec.load('word2vec_solved_iter__noshuf_5window_100_size_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('CI23663', 0.3396339416503906),\n",
       " ('CI23975', 0.32559213042259216),\n",
       " ('CI23933', 0.31812340021133423),\n",
       " ('CI25135', 0.29852694272994995),\n",
       " ('CI24915', 0.283422589302063),\n",
       " ('CI23913', 0.2829606831073761),\n",
       " ('CI25125', 0.2683682441711426),\n",
       " ('CI23714', 0.2660852074623108),\n",
       " ('CI24527', 0.2611076831817627),\n",
       " ('CI24958', 0.2601049542427063)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('CI23855')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import Merge, TimeDistributed, concatenate, Bidirectional, Masking, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Result embedding shape:', (5502, 500))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "  return model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate int sequences for challenge sentences\n",
    "train_seq=np.array(train.challenge.apply(lambda x: x.split()).apply(lambda x: ([(word2idx(y)) for y in x])).tolist())\n",
    "test_seq=np.array(test.challenge.apply(lambda x: x.split()).apply(lambda x: ([(word2idx(y)) for y in x])).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69532, 10), (69532,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain=train_seq[:,:10]\n",
    "ytrain= train_seq[:,10]\n",
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add 11th,12th,13th challenge to outcome\n",
    "xtrain= np.concatenate((xtrain,xtrain,xtrain),axis=0)\n",
    "ytrain= np.concatenate((train_seq[:,10],train_seq[:,11],train_seq[:,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define nn model\n",
    "emdedding_size=500\n",
    "vocab_size=5502\n",
    "keras_model2 = Sequential()\n",
    "keras_model2.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, \n",
    "                    weights=[pretrained_weights]))\n",
    "keras_model2.add(GRU(units=emdedding_size))\n",
    "keras_model2.add(Dense(units=vocab_size))\n",
    "\n",
    "keras_model2.add(Activation('softmax'))\n",
    "keras_model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weight=np.concatenate((np.ones(train.shape[0])*3,np.ones(train.shape[0])*2,np.ones(train.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1024/208596 [..............................] - ETA: 938s - loss: 17.0322 "
     ]
    }
   ],
   "source": [
    "# fit differnt samples of data and then full data\n",
    "keras_model2.fit(xtrain,ytrain, epochs=2, sample_weight=sample_weight,\n",
    "          batch_size=512, verbose=1,validation_split=0.11)\n",
    "keras_model2.fit(xtrain,ytrain, epochs=1, sample_weight=sample_weight,\n",
    "          batch_size=512, verbose=1,validation_split=0.11)\n",
    "keras_model2.fit(xtrain,ytrain, epochs=1, sample_weight=sample_weight,\n",
    "          batch_size=512, verbose=1,validation_split=0.11)\n",
    "keras_model2.fit(xtrain,ytrain, epochs=1, sample_weight=sample_weight,\n",
    "          batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val loss= 5.0348, 5.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(keras_model2.to_json(),open('gru4.pkl','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import keras\n",
    "# from importlib import reload\n",
    "# import keras\n",
    "keras_model2.save_weights('gru4.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return preds.argsort()[-3:][::-1]\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "#     print max(probas[0])\n",
    "    retar= probas[0].argsort()[-3:][::-1]\n",
    "    return retar\n",
    "\n",
    "def generate_next(text):\n",
    "    global count\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "    word_idxs = [word2idx(word) for word in text.split()]\n",
    "    prediction = keras_model2.predict(x=np.array(word_idxs))\n",
    "#     print prediction\n",
    "    idxl = sample(prediction[-1], temperature=0.0)\n",
    "#     word_idxs.e(idx)\n",
    "#     print idxl\n",
    "    return [idx2word(idx) for idx in idxl]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CI23855 CI23933 CI24917 CI24915 CI23714 CI23663 CI24958 CI25135 CI25727 CI24530'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.challenge.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CI26215', 'CI26903', 'CI26221']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_next('CI23855 CI23933 CI24917 CI24915 CI23714 CI23663 CI24958 CI25135 CI25727 CI24530')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_inds=np.array(test.challenge.apply(lambda x: np.array([word2idx(word) for word in x.split()])).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39732, 10), (69532, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inds.shape, xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "nnpred=keras_model2.predict(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39732, 5502)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnpred2= pd.Series(nnpred.argsort(axis=1)[:,-3:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store to csv\n",
    "count=0\n",
    "test['pred']=nnpred2.apply(lambda x: [idx2word(y) for y in x[::-1]])\n",
    "test_sub= test.copy()\n",
    "test_sub=pd.concat((test_sub,test_sub,test_sub))\n",
    "test_sub=test_sub.sort_values('user_id').reset_index(drop=True)\n",
    "test_sub['seq']=test_sub.groupby('user_id').cumcount()\n",
    "test_sub.seq= test_sub.seq+11\n",
    "test_sub['user_sequence']= test_sub.user_id.astype('str') + '_' + test_sub.seq.astype('str')\n",
    "test_sub['challenge']=test_sub.apply(lambda row: row['pred'][row['seq']-11], axis=1)\n",
    "test_sub[['user_sequence','challenge']].to_csv('ksubmission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
